{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3845097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import models\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from utils import cal_param_size, cal_multi_adds\n",
    "\n",
    "\n",
    "from bisect import bisect_right\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dc4b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "checkpoint_dir = './checkpoint'\n",
    "dataset = 'cifar100'\n",
    "arch = 'wrn_16_2_aux'\n",
    "tarch = 'wrn_40_2_aux'\n",
    "tcheckpoint = './checkpoint/wrn_40_2_aux.pth.tar'\n",
    "init_lr = 0.05\n",
    "weight_decay = 5e-4\n",
    "lr_type = 'multistep'\n",
    "resume = False\n",
    "evaluate = False\n",
    "milestones = [150, 180, 210]\n",
    "sgdr_t = 300\n",
    "warmup_epoch = 0\n",
    "epochs = 240\n",
    "batch_size = 64\n",
    "num_workers = 8\n",
    "gpu_id = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id\n",
    "manual_seed = 0\n",
    "kd_T = 3\n",
    "num_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82e17ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "torch.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cd82685",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_txt = 'result/'+ str(os.path.basename('kushagrabhushan/TrAIL/WideResNet').split('.')[0]) + '_'+\\\n",
    "          'tarch' + '_' +  tarch + '_'+\\\n",
    "          'arch' + '_' +  arch + '_'+\\\n",
    "          'dataset' + '_' +  dataset + '_'+\\\n",
    "          'seed'+ str(manual_seed) +'.txt'\n",
    "\n",
    "log_dir = str(os.path.basename('kushagrabhushan/TrAIL/WideResNet').split('.')[0]) + '_'+\\\n",
    "          'tarch' + '_' +  tarch + '_'+\\\n",
    "          'arch'+ '_' + arch + '_'+\\\n",
    "          'dataset' + '_' +  dataset + '_'+\\\n",
    "          'seed'+ str(manual_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de0308b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "trainset = torchvision.datasets.CIFAR100(root=data_path, train=True, download=True,\n",
    "                                        transform=transforms.Compose([\n",
    "                                            transforms.RandomCrop(32, padding=4),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize([0.5071, 0.4867, 0.4408],\n",
    "                                                                [0.2675, 0.2565, 0.2761])\n",
    "                                        ]))\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root=data_path, train=False, download=True,\n",
    "                                        transform=transforms.Compose([\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize([0.5071, 0.4867, 0.4408],\n",
    "                                                                [0.2675, 0.2565, 0.2761]),\n",
    "                                        ]))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True,\n",
    "                                    pin_memory=(torch.cuda.is_available()))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False,\n",
    "                                    pin_memory=(torch.cuda.is_available()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9d07599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pre-trained teacher weights from: ./checkpoint/wrn_40_2_aux.pth.tar\n"
     ]
    }
   ],
   "source": [
    "print('load pre-trained teacher weights from: {}'.format(tcheckpoint))     \n",
    "checkpoint = torch.load(tcheckpoint, map_location=torch.device('cpu'))\n",
    "\n",
    "model = getattr(models, arch)\n",
    "net = model(num_classes=num_classes).cuda()\n",
    "net =  torch.nn.DataParallel(net)\n",
    "\n",
    "tmodel = getattr(models, tarch)\n",
    "tnet = tmodel(num_classes=num_classes).cuda()\n",
    "tnet.load_state_dict(checkpoint['net'])\n",
    "tnet.eval()\n",
    "tnet =  torch.nn.DataParallel(tnet)\n",
    "\n",
    "_, ss_logits = net(torch.randn(2, 3, 32, 32))\n",
    "num_auxiliary_branches = len(ss_logits)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d51a6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillKL(nn.Module):\n",
    "    \"\"\"Distilling the Knowledge in a Neural Network\"\"\"\n",
    "    def __init__(self, T):\n",
    "        super(DistillKL, self).__init__()\n",
    "        self.T = T\n",
    "\n",
    "    def forward(self, y_s, y_t):\n",
    "        p_s = F.log_softmax(y_s/self.T, dim=1)\n",
    "        p_t = F.softmax(y_t/self.T, dim=1)\n",
    "        loss = F.kl_div(p_s, p_t, reduction='batchmean') * (self.T**2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1b39a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_num(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    correct = pred.eq(target.view(-1, 1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:, :k].float().sum()\n",
    "        res.append(correct_k)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05bb2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lr(optimizer, epoch, args, step=0, all_iters_per_epoch=0):\n",
    "    cur_lr = 0.\n",
    "    if epoch < args['warmup_epoch']:\n",
    "        cur_lr = args['init_lr'] * float(1 + step + epoch*all_iters_per_epoch)/(warmup_epoch *all_iters_per_epoch)\n",
    "    else:\n",
    "        epoch = epoch - args['warmup_epoch']\n",
    "        cur_lr = args['init_lr'] * 0.1 ** bisect_right(args['milestones'], epoch)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = cur_lr\n",
    "    return cur_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fe3e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, criterion_list, optimizer):\n",
    "    train_loss = 0.\n",
    "    train_loss_cls = 0.\n",
    "    train_loss_div = 0.\n",
    "\n",
    "    ss_top1_num = [0] * num_auxiliary_branches\n",
    "    ss_top5_num = [0] * num_auxiliary_branches\n",
    "    class_top1_num = [0] * num_auxiliary_branches\n",
    "    class_top5_num = [0] * num_auxiliary_branches\n",
    "    top1_num = 0\n",
    "    top5_num = 0\n",
    "    total = 0\n",
    "\n",
    "    if epoch >= warmup_epoch:\n",
    "        lr = adjust_lr(optimizer, epoch, args)\n",
    "\n",
    "    start_time = time.time()\n",
    "    criterion_cls = criterion_list[0]\n",
    "    criterion_div = criterion_list[1]\n",
    "\n",
    "    net.train()\n",
    "    for batch_idx, (input, target) in enumerate(trainloader):\n",
    "        batch_start_time = time.time()\n",
    "        input = input.float().cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        size = input.shape[1:]\n",
    "        input = torch.stack([torch.rot90(input, k, (2, 3)) for k in range(4)], 1).view(-1, *size)\n",
    "        labels = torch.stack([target*4+i for i in range(4)], 1).view(-1)\n",
    "\n",
    "        if epoch < warmup_epoch:\n",
    "            lr = adjust_lr(optimizer, epoch, args, batch_idx, len(trainloader))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, ss_logits = net(input, grad=True)\n",
    "        with torch.no_grad():\n",
    "            t_logits, t_ss_logits = tnet(input)\n",
    "\n",
    "        loss_cls = torch.tensor(0.).cuda()\n",
    "        loss_div = torch.tensor(0.).cuda()\n",
    "\n",
    "        loss_cls = loss_cls + criterion_cls(logits[0::4], target)\n",
    "        for i in range(len(ss_logits)):\n",
    "            loss_div = loss_div + criterion_div(ss_logits[i], t_ss_logits[i].detach())\n",
    "        \n",
    "        loss_div = loss_div + criterion_div(logits, t_logits.detach())\n",
    "        \n",
    "            \n",
    "        loss = loss_cls + loss_div\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        train_loss += loss.item() / len(trainloader)\n",
    "        train_loss_cls += loss_cls.item() / len(trainloader)\n",
    "        train_loss_div += loss_div.item() / len(trainloader)\n",
    "\n",
    "        for i in range(len(ss_logits)):\n",
    "            top1, top5 = correct_num(ss_logits[i], labels, topk=(1, 5))\n",
    "            ss_top1_num[i] += top1\n",
    "            ss_top5_num[i] += top5\n",
    "        \n",
    "        class_logits = [torch.stack(torch.split(ss_logits[i], split_size_or_sections=4, dim=1), dim=1).sum(dim=2) for i in range(len(ss_logits))]\n",
    "        multi_target = target.view(-1, 1).repeat(1, 4).view(-1)\n",
    "        for i in range(len(class_logits)):\n",
    "            top1, top5 = correct_num(class_logits[i], multi_target, topk=(1, 5))\n",
    "            class_top1_num[i] += top1\n",
    "            class_top5_num[i] += top5\n",
    "\n",
    "        logits = logits.view(-1, 4, num_classes)[:, 0, :]\n",
    "        top1, top5 = correct_num(logits, target, topk=(1, 5))\n",
    "        top1_num += top1\n",
    "        top5_num += top5\n",
    "        total += target.size(0)\n",
    "\n",
    "        print('Epoch:{}, batch_idx:{}/{}, lr:{:.5f}, Duration:{:.2f}, Top-1 Acc:{:.4f}'.format(\n",
    "            epoch, batch_idx, len(trainloader), lr, time.time()-batch_start_time, (top1_num/(total)).item()))\n",
    "\n",
    "\n",
    "    ss_acc1 = [round((ss_top1_num[i]/(total*4)).item(), 4) for i in range(num_auxiliary_branches)]\n",
    "    ss_acc5 = [round((ss_top5_num[i]/(total*4)).item(), 4) for i in range(num_auxiliary_branches)]\n",
    "    class_acc1 = [round((class_top1_num[i]/(total*4)).item(), 4) for i in range(num_auxiliary_branches)] + [round((top1_num/(total)).item(), 4)]\n",
    "    class_acc5 = [round((class_top5_num[i]/(total*4)).item(), 4) for i in range(num_auxiliary_branches)] + [round((top5_num/(total)).item(), 4)]\n",
    "    \n",
    "    print('Train epoch:{}\\nTrain Top-1 ss_accuracy: {}\\nTrain Top-1 class_accuracy: {}\\n'.format(epoch, str(ss_acc1), str(class_acc1)))\n",
    "\n",
    "    with open(log_txt, 'a+') as f:\n",
    "        f.write('Epoch:{}\\t lr:{:.5f}\\t duration:{:.3f}'\n",
    "                '\\n train_loss:{:.5f}\\t train_loss_cls:{:.5f}\\t train_loss_div:{:.5f}'\n",
    "                '\\nTrain Top-1 ss_accuracy: {}\\nTrain Top-1 class_accuracy: {}\\n'\n",
    "                .format(epoch, lr, time.time() - start_time,\n",
    "                        train_loss, train_loss_cls, train_loss_div,\n",
    "                        str(ss_acc1), str(class_acc1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64ac1f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, criterion_cls, net):\n",
    "    global best_acc\n",
    "    test_loss_cls = 0.\n",
    "\n",
    "    ss_top1_num = [0] * (num_auxiliary_branches)\n",
    "    ss_top5_num = [0] * (num_auxiliary_branches)\n",
    "    class_top1_num = [0] * num_auxiliary_branches\n",
    "    class_top5_num = [0] * num_auxiliary_branches\n",
    "    top1_num = 0\n",
    "    top5_num = 0\n",
    "    total = 0\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, target) in enumerate(testloader):\n",
    "            batch_start_time = time.time()\n",
    "            input, target = inputs.cuda(), target.cuda()\n",
    "\n",
    "            size = input.shape[1:]\n",
    "            input = torch.stack([torch.rot90(input, k, (2, 3)) for k in range(4)], 1).view(-1, *size)\n",
    "            labels = torch.stack([target*4+i for i in range(4)], 1).view(-1)\n",
    "            \n",
    "            logits, ss_logits = net(input)\n",
    "            loss_cls = torch.tensor(0.).cuda()\n",
    "            loss_cls = loss_cls + criterion_cls(logits[0::4], target)\n",
    "\n",
    "            test_loss_cls += loss_cls.item()/ len(testloader)\n",
    "\n",
    "            batch_size = logits.size(0) // 4\n",
    "            for i in range(len(ss_logits)):\n",
    "                top1, top5 = correct_num(ss_logits[i], labels, topk=(1, 5))\n",
    "                ss_top1_num[i] += top1\n",
    "                ss_top5_num[i] += top5\n",
    "                \n",
    "            class_logits = [torch.stack(torch.split(ss_logits[i], split_size_or_sections=4, dim=1), dim=1).sum(dim=2) for i in range(len(ss_logits))]\n",
    "            multi_target = target.view(-1, 1).repeat(1, 4).view(-1)\n",
    "            for i in range(len(class_logits)):\n",
    "                top1, top5 = correct_num(class_logits[i], multi_target, topk=(1, 5))\n",
    "                class_top1_num[i] += top1\n",
    "                class_top5_num[i] += top5\n",
    "\n",
    "            logits = logits.view(-1, 4, num_classes)[:, 0, :]\n",
    "            top1, top5 = correct_num(logits, target, topk=(1, 5))\n",
    "            top1_num += top1\n",
    "            top5_num += top5\n",
    "            total += target.size(0)\n",
    "            \n",
    "\n",
    "            print('Epoch:{}, batch_idx:{}/{}, Duration:{:.2f}, Top-1 Acc:{:.4f}'.format(\n",
    "                epoch, batch_idx, len(testloader), time.time()-batch_start_time, (top1_num/(total)).item()))\n",
    "\n",
    "        ss_acc1 = [round((ss_top1_num[i]/(total*4)).item(), 4) for i in range(len(ss_logits))]\n",
    "        ss_acc5 = [round((ss_top5_num[i]/(total*4)).item(), 4) for i in range(len(ss_logits))]\n",
    "        class_acc1 = [round((class_top1_num[i]/(total*4)).item(), 4) for i in range(num_auxiliary_branches)] + [round((top1_num/(total)).item(), 4)]\n",
    "        class_acc5 = [round((class_top5_num[i]/(total*4)).item(), 4) for i in range(num_auxiliary_branches)] + [round((top5_num/(total)).item(), 4)]\n",
    "        with open(log_txt, 'a+') as f:\n",
    "            f.write('test epoch:{}\\t test_loss_cls:{:.5f}\\nTop-1 ss_accuracy: {}\\nTop-1 class_accuracy: {}\\n'\n",
    "                    .format(epoch, test_loss_cls, str(ss_acc1), str(class_acc1)))\n",
    "        print('test epoch:{}\\nTest Top-1 ss_accuracy: {}\\nTest Top-1 class_accuracy: {}\\n'.format(epoch, str(ss_acc1), str(class_acc1)))\n",
    "\n",
    "    return class_acc1[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "494df40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Teacher:\n",
      "Epoch:0, batch_idx:0/157, Duration:0.05, Top-1 Acc:0.7812\n",
      "Epoch:0, batch_idx:1/157, Duration:0.04, Top-1 Acc:0.8047\n",
      "Epoch:0, batch_idx:2/157, Duration:0.04, Top-1 Acc:0.7865\n",
      "Epoch:0, batch_idx:3/157, Duration:0.04, Top-1 Acc:0.8008\n",
      "Epoch:0, batch_idx:4/157, Duration:0.04, Top-1 Acc:0.8000\n",
      "Epoch:0, batch_idx:5/157, Duration:0.04, Top-1 Acc:0.7995\n",
      "Epoch:0, batch_idx:6/157, Duration:0.04, Top-1 Acc:0.7924\n",
      "Epoch:0, batch_idx:7/157, Duration:0.04, Top-1 Acc:0.8027\n",
      "Epoch:0, batch_idx:8/157, Duration:0.04, Top-1 Acc:0.7830\n",
      "Epoch:0, batch_idx:9/157, Duration:0.04, Top-1 Acc:0.7891\n",
      "Epoch:0, batch_idx:10/157, Duration:0.04, Top-1 Acc:0.7926\n",
      "Epoch:0, batch_idx:11/157, Duration:0.04, Top-1 Acc:0.7956\n",
      "Epoch:0, batch_idx:12/157, Duration:0.04, Top-1 Acc:0.7993\n",
      "Epoch:0, batch_idx:13/157, Duration:0.04, Top-1 Acc:0.7991\n",
      "Epoch:0, batch_idx:14/157, Duration:0.04, Top-1 Acc:0.7948\n",
      "Epoch:0, batch_idx:15/157, Duration:0.04, Top-1 Acc:0.7900\n",
      "Epoch:0, batch_idx:16/157, Duration:0.04, Top-1 Acc:0.7868\n",
      "Epoch:0, batch_idx:17/157, Duration:0.04, Top-1 Acc:0.7882\n",
      "Epoch:0, batch_idx:18/157, Duration:0.04, Top-1 Acc:0.7895\n",
      "Epoch:0, batch_idx:19/157, Duration:0.04, Top-1 Acc:0.7930\n",
      "Epoch:0, batch_idx:20/157, Duration:0.04, Top-1 Acc:0.7924\n",
      "Epoch:0, batch_idx:21/157, Duration:0.04, Top-1 Acc:0.7926\n",
      "Epoch:0, batch_idx:22/157, Duration:0.04, Top-1 Acc:0.7955\n",
      "Epoch:0, batch_idx:23/157, Duration:0.04, Top-1 Acc:0.7956\n",
      "Epoch:0, batch_idx:24/157, Duration:0.04, Top-1 Acc:0.7956\n",
      "Epoch:0, batch_idx:25/157, Duration:0.04, Top-1 Acc:0.7951\n",
      "Epoch:0, batch_idx:26/157, Duration:0.04, Top-1 Acc:0.7951\n",
      "Epoch:0, batch_idx:27/157, Duration:0.04, Top-1 Acc:0.7963\n",
      "Epoch:0, batch_idx:28/157, Duration:0.04, Top-1 Acc:0.7931\n",
      "Epoch:0, batch_idx:29/157, Duration:0.04, Top-1 Acc:0.7953\n",
      "Epoch:0, batch_idx:30/157, Duration:0.04, Top-1 Acc:0.7979\n",
      "Epoch:0, batch_idx:31/157, Duration:0.04, Top-1 Acc:0.8003\n",
      "Epoch:0, batch_idx:32/157, Duration:0.04, Top-1 Acc:0.7988\n",
      "Epoch:0, batch_idx:33/157, Duration:0.04, Top-1 Acc:0.7996\n",
      "Epoch:0, batch_idx:34/157, Duration:0.04, Top-1 Acc:0.7982\n",
      "Epoch:0, batch_idx:35/157, Duration:0.04, Top-1 Acc:0.7990\n",
      "Epoch:0, batch_idx:36/157, Duration:0.04, Top-1 Acc:0.7977\n",
      "Epoch:0, batch_idx:37/157, Duration:0.04, Top-1 Acc:0.7981\n",
      "Epoch:0, batch_idx:38/157, Duration:0.04, Top-1 Acc:0.7985\n",
      "Epoch:0, batch_idx:39/157, Duration:0.04, Top-1 Acc:0.7996\n",
      "Epoch:0, batch_idx:40/157, Duration:0.04, Top-1 Acc:0.7992\n",
      "Epoch:0, batch_idx:41/157, Duration:0.04, Top-1 Acc:0.7980\n",
      "Epoch:0, batch_idx:42/157, Duration:0.04, Top-1 Acc:0.7991\n",
      "Epoch:0, batch_idx:43/157, Duration:0.04, Top-1 Acc:0.7994\n",
      "Epoch:0, batch_idx:44/157, Duration:0.04, Top-1 Acc:0.7986\n",
      "Epoch:0, batch_idx:45/157, Duration:0.04, Top-1 Acc:0.7986\n",
      "Epoch:0, batch_idx:46/157, Duration:0.04, Top-1 Acc:0.7985\n",
      "Epoch:0, batch_idx:47/157, Duration:0.04, Top-1 Acc:0.7979\n",
      "Epoch:0, batch_idx:48/157, Duration:0.04, Top-1 Acc:0.7969\n",
      "Epoch:0, batch_idx:49/157, Duration:0.04, Top-1 Acc:0.7966\n",
      "Epoch:0, batch_idx:50/157, Duration:0.04, Top-1 Acc:0.7956\n",
      "Epoch:0, batch_idx:51/157, Duration:0.04, Top-1 Acc:0.7954\n",
      "Epoch:0, batch_idx:52/157, Duration:0.04, Top-1 Acc:0.7951\n",
      "Epoch:0, batch_idx:53/157, Duration:0.04, Top-1 Acc:0.7948\n",
      "Epoch:0, batch_idx:54/157, Duration:0.04, Top-1 Acc:0.7940\n",
      "Epoch:0, batch_idx:55/157, Duration:0.04, Top-1 Acc:0.7919\n",
      "Epoch:0, batch_idx:56/157, Duration:0.04, Top-1 Acc:0.7925\n",
      "Epoch:0, batch_idx:57/157, Duration:0.04, Top-1 Acc:0.7936\n",
      "Epoch:0, batch_idx:58/157, Duration:0.04, Top-1 Acc:0.7945\n",
      "Epoch:0, batch_idx:59/157, Duration:0.04, Top-1 Acc:0.7943\n",
      "Epoch:0, batch_idx:60/157, Duration:0.04, Top-1 Acc:0.7928\n",
      "Epoch:0, batch_idx:61/157, Duration:0.04, Top-1 Acc:0.7931\n",
      "Epoch:0, batch_idx:62/157, Duration:0.04, Top-1 Acc:0.7937\n",
      "Epoch:0, batch_idx:63/157, Duration:0.04, Top-1 Acc:0.7930\n",
      "Epoch:0, batch_idx:64/157, Duration:0.04, Top-1 Acc:0.7935\n",
      "Epoch:0, batch_idx:65/157, Duration:0.04, Top-1 Acc:0.7926\n",
      "Epoch:0, batch_idx:66/157, Duration:0.04, Top-1 Acc:0.7941\n",
      "Epoch:0, batch_idx:67/157, Duration:0.04, Top-1 Acc:0.7937\n",
      "Epoch:0, batch_idx:68/157, Duration:0.04, Top-1 Acc:0.7944\n",
      "Epoch:0, batch_idx:69/157, Duration:0.04, Top-1 Acc:0.7949\n",
      "Epoch:0, batch_idx:70/157, Duration:0.04, Top-1 Acc:0.7942\n",
      "Epoch:0, batch_idx:71/157, Duration:0.04, Top-1 Acc:0.7941\n",
      "Epoch:0, batch_idx:72/157, Duration:0.04, Top-1 Acc:0.7947\n",
      "Epoch:0, batch_idx:73/157, Duration:0.04, Top-1 Acc:0.7935\n",
      "Epoch:0, batch_idx:74/157, Duration:0.04, Top-1 Acc:0.7935\n",
      "Epoch:0, batch_idx:75/157, Duration:0.04, Top-1 Acc:0.7934\n",
      "Epoch:0, batch_idx:76/157, Duration:0.04, Top-1 Acc:0.7938\n",
      "Epoch:0, batch_idx:77/157, Duration:0.04, Top-1 Acc:0.7947\n",
      "Epoch:0, batch_idx:78/157, Duration:0.04, Top-1 Acc:0.7939\n",
      "Epoch:0, batch_idx:79/157, Duration:0.04, Top-1 Acc:0.7924\n",
      "Epoch:0, batch_idx:80/157, Duration:0.04, Top-1 Acc:0.7917\n",
      "Epoch:0, batch_idx:81/157, Duration:0.04, Top-1 Acc:0.7921\n",
      "Epoch:0, batch_idx:82/157, Duration:0.04, Top-1 Acc:0.7912\n",
      "Epoch:0, batch_idx:83/157, Duration:0.04, Top-1 Acc:0.7917\n",
      "Epoch:0, batch_idx:84/157, Duration:0.04, Top-1 Acc:0.7921\n",
      "Epoch:0, batch_idx:85/157, Duration:0.04, Top-1 Acc:0.7920\n",
      "Epoch:0, batch_idx:86/157, Duration:0.04, Top-1 Acc:0.7926\n",
      "Epoch:0, batch_idx:87/157, Duration:0.04, Top-1 Acc:0.7930\n",
      "Epoch:0, batch_idx:88/157, Duration:0.04, Top-1 Acc:0.7920\n",
      "Epoch:0, batch_idx:89/157, Duration:0.04, Top-1 Acc:0.7920\n",
      "Epoch:0, batch_idx:90/157, Duration:0.04, Top-1 Acc:0.7926\n",
      "Epoch:0, batch_idx:91/157, Duration:0.04, Top-1 Acc:0.7926\n",
      "Epoch:0, batch_idx:92/157, Duration:0.04, Top-1 Acc:0.7925\n",
      "Epoch:0, batch_idx:93/157, Duration:0.04, Top-1 Acc:0.7924\n",
      "Epoch:0, batch_idx:94/157, Duration:0.04, Top-1 Acc:0.7923\n",
      "Epoch:0, batch_idx:95/157, Duration:0.04, Top-1 Acc:0.7918\n",
      "Epoch:0, batch_idx:96/157, Duration:0.04, Top-1 Acc:0.7917\n",
      "Epoch:0, batch_idx:97/157, Duration:0.04, Top-1 Acc:0.7919\n",
      "Epoch:0, batch_idx:98/157, Duration:0.04, Top-1 Acc:0.7918\n",
      "Epoch:0, batch_idx:99/157, Duration:0.04, Top-1 Acc:0.7922\n",
      "Epoch:0, batch_idx:100/157, Duration:0.04, Top-1 Acc:0.7924\n",
      "Epoch:0, batch_idx:101/157, Duration:0.04, Top-1 Acc:0.7914\n",
      "Epoch:0, batch_idx:102/157, Duration:0.04, Top-1 Acc:0.7897\n",
      "Epoch:0, batch_idx:103/157, Duration:0.04, Top-1 Acc:0.7894\n",
      "Epoch:0, batch_idx:104/157, Duration:0.04, Top-1 Acc:0.7894\n",
      "Epoch:0, batch_idx:105/157, Duration:0.04, Top-1 Acc:0.7898\n",
      "Epoch:0, batch_idx:106/157, Duration:0.04, Top-1 Acc:0.7900\n",
      "Epoch:0, batch_idx:107/157, Duration:0.04, Top-1 Acc:0.7905\n",
      "Epoch:0, batch_idx:108/157, Duration:0.04, Top-1 Acc:0.7909\n",
      "Epoch:0, batch_idx:109/157, Duration:0.04, Top-1 Acc:0.7912\n",
      "Epoch:0, batch_idx:110/157, Duration:0.04, Top-1 Acc:0.7907\n",
      "Epoch:0, batch_idx:111/157, Duration:0.04, Top-1 Acc:0.7910\n",
      "Epoch:0, batch_idx:112/157, Duration:0.04, Top-1 Acc:0.7918\n",
      "Epoch:0, batch_idx:113/157, Duration:0.04, Top-1 Acc:0.7924\n",
      "Epoch:0, batch_idx:114/157, Duration:0.04, Top-1 Acc:0.7921\n",
      "Epoch:0, batch_idx:115/157, Duration:0.04, Top-1 Acc:0.7926\n",
      "Epoch:0, batch_idx:116/157, Duration:0.04, Top-1 Acc:0.7915\n",
      "Epoch:0, batch_idx:117/157, Duration:0.04, Top-1 Acc:0.7922\n",
      "Epoch:0, batch_idx:118/157, Duration:0.04, Top-1 Acc:0.7916\n",
      "Epoch:0, batch_idx:119/157, Duration:0.04, Top-1 Acc:0.7914\n",
      "Epoch:0, batch_idx:120/157, Duration:0.04, Top-1 Acc:0.7916\n",
      "Epoch:0, batch_idx:121/157, Duration:0.04, Top-1 Acc:0.7910\n",
      "Epoch:0, batch_idx:122/157, Duration:0.04, Top-1 Acc:0.7915\n",
      "Epoch:0, batch_idx:123/157, Duration:0.04, Top-1 Acc:0.7912\n",
      "Epoch:0, batch_idx:124/157, Duration:0.04, Top-1 Acc:0.7909\n",
      "Epoch:0, batch_idx:125/157, Duration:0.04, Top-1 Acc:0.7906\n",
      "Epoch:0, batch_idx:126/157, Duration:0.04, Top-1 Acc:0.7910\n",
      "Epoch:0, batch_idx:127/157, Duration:0.04, Top-1 Acc:0.7919\n",
      "Epoch:0, batch_idx:128/157, Duration:0.04, Top-1 Acc:0.7920\n",
      "Epoch:0, batch_idx:129/157, Duration:0.04, Top-1 Acc:0.7919\n",
      "Epoch:0, batch_idx:130/157, Duration:0.04, Top-1 Acc:0.7915\n",
      "Epoch:0, batch_idx:131/157, Duration:0.04, Top-1 Acc:0.7912\n",
      "Epoch:0, batch_idx:132/157, Duration:0.04, Top-1 Acc:0.7914\n",
      "Epoch:0, batch_idx:133/157, Duration:0.04, Top-1 Acc:0.7919\n",
      "Epoch:0, batch_idx:134/157, Duration:0.04, Top-1 Acc:0.7914\n",
      "Epoch:0, batch_idx:135/157, Duration:0.04, Top-1 Acc:0.7918\n",
      "Epoch:0, batch_idx:136/157, Duration:0.04, Top-1 Acc:0.7925\n",
      "Epoch:0, batch_idx:137/157, Duration:0.04, Top-1 Acc:0.7928\n",
      "Epoch:0, batch_idx:138/157, Duration:0.04, Top-1 Acc:0.7931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, batch_idx:139/157, Duration:0.04, Top-1 Acc:0.7932\n",
      "Epoch:0, batch_idx:140/157, Duration:0.04, Top-1 Acc:0.7933\n",
      "Epoch:0, batch_idx:141/157, Duration:0.04, Top-1 Acc:0.7928\n",
      "Epoch:0, batch_idx:142/157, Duration:0.04, Top-1 Acc:0.7922\n",
      "Epoch:0, batch_idx:143/157, Duration:0.04, Top-1 Acc:0.7926\n",
      "Epoch:0, batch_idx:144/157, Duration:0.04, Top-1 Acc:0.7926\n",
      "Epoch:0, batch_idx:145/157, Duration:0.04, Top-1 Acc:0.7925\n",
      "Epoch:0, batch_idx:146/157, Duration:0.04, Top-1 Acc:0.7927\n",
      "Epoch:0, batch_idx:147/157, Duration:0.04, Top-1 Acc:0.7927\n",
      "Epoch:0, batch_idx:148/157, Duration:0.04, Top-1 Acc:0.7936\n",
      "Epoch:0, batch_idx:149/157, Duration:0.04, Top-1 Acc:0.7934\n",
      "Epoch:0, batch_idx:150/157, Duration:0.04, Top-1 Acc:0.7936\n",
      "Epoch:0, batch_idx:151/157, Duration:0.04, Top-1 Acc:0.7939\n",
      "Epoch:0, batch_idx:152/157, Duration:0.04, Top-1 Acc:0.7935\n",
      "Epoch:0, batch_idx:153/157, Duration:0.04, Top-1 Acc:0.7938\n",
      "Epoch:0, batch_idx:154/157, Duration:0.04, Top-1 Acc:0.7941\n",
      "Epoch:0, batch_idx:155/157, Duration:0.04, Top-1 Acc:0.7941\n",
      "Epoch:0, batch_idx:156/157, Duration:0.02, Top-1 Acc:0.7942\n",
      "test epoch:0\n",
      "Test Top-1 ss_accuracy: [0.6615, 0.6628, 0.6651]\n",
      "Test Top-1 class_accuracy: [0.6757, 0.6794, 0.6918, 0.7942]\n",
      "\n",
      "Teacher Acc: 0.7942\n",
      "load pre-trained weights from: ./checkpoint/wrn_16_2_aux.pth.tar\n",
      "240\n",
      "Evaluate the best model:\n",
      "load pre-trained weights from: ./checkpoint/wrn_16_2_aux_best.pth.tar\n",
      "Epoch:234, batch_idx:0/157, Duration:0.02, Top-1 Acc:0.7812\n",
      "Epoch:234, batch_idx:1/157, Duration:0.02, Top-1 Acc:0.7969\n",
      "Epoch:234, batch_idx:2/157, Duration:0.02, Top-1 Acc:0.8073\n",
      "Epoch:234, batch_idx:3/157, Duration:0.02, Top-1 Acc:0.8047\n",
      "Epoch:234, batch_idx:4/157, Duration:0.02, Top-1 Acc:0.8094\n",
      "Epoch:234, batch_idx:5/157, Duration:0.02, Top-1 Acc:0.8047\n",
      "Epoch:234, batch_idx:6/157, Duration:0.02, Top-1 Acc:0.7902\n",
      "Epoch:234, batch_idx:7/157, Duration:0.02, Top-1 Acc:0.7988\n",
      "Epoch:234, batch_idx:8/157, Duration:0.02, Top-1 Acc:0.7847\n",
      "Epoch:234, batch_idx:9/157, Duration:0.02, Top-1 Acc:0.7859\n",
      "Epoch:234, batch_idx:10/157, Duration:0.02, Top-1 Acc:0.7869\n",
      "Epoch:234, batch_idx:11/157, Duration:0.02, Top-1 Acc:0.7917\n",
      "Epoch:234, batch_idx:12/157, Duration:0.02, Top-1 Acc:0.7945\n",
      "Epoch:234, batch_idx:13/157, Duration:0.02, Top-1 Acc:0.7891\n",
      "Epoch:234, batch_idx:14/157, Duration:0.02, Top-1 Acc:0.7844\n",
      "Epoch:234, batch_idx:15/157, Duration:0.02, Top-1 Acc:0.7812\n",
      "Epoch:234, batch_idx:16/157, Duration:0.02, Top-1 Acc:0.7776\n",
      "Epoch:234, batch_idx:17/157, Duration:0.02, Top-1 Acc:0.7778\n",
      "Epoch:234, batch_idx:18/157, Duration:0.02, Top-1 Acc:0.7788\n",
      "Epoch:234, batch_idx:19/157, Duration:0.02, Top-1 Acc:0.7836\n",
      "Epoch:234, batch_idx:20/157, Duration:0.02, Top-1 Acc:0.7842\n",
      "Epoch:234, batch_idx:21/157, Duration:0.02, Top-1 Acc:0.7855\n",
      "Epoch:234, batch_idx:22/157, Duration:0.02, Top-1 Acc:0.7874\n",
      "Epoch:234, batch_idx:23/157, Duration:0.02, Top-1 Acc:0.7852\n",
      "Epoch:234, batch_idx:24/157, Duration:0.02, Top-1 Acc:0.7856\n",
      "Epoch:234, batch_idx:25/157, Duration:0.02, Top-1 Acc:0.7837\n",
      "Epoch:234, batch_idx:26/157, Duration:0.02, Top-1 Acc:0.7812\n",
      "Epoch:234, batch_idx:27/157, Duration:0.02, Top-1 Acc:0.7801\n",
      "Epoch:234, batch_idx:28/157, Duration:0.02, Top-1 Acc:0.7775\n",
      "Epoch:234, batch_idx:29/157, Duration:0.02, Top-1 Acc:0.7786\n",
      "Epoch:234, batch_idx:30/157, Duration:0.02, Top-1 Acc:0.7812\n",
      "Epoch:234, batch_idx:31/157, Duration:0.02, Top-1 Acc:0.7842\n",
      "Epoch:234, batch_idx:32/157, Duration:0.02, Top-1 Acc:0.7831\n",
      "Epoch:234, batch_idx:33/157, Duration:0.02, Top-1 Acc:0.7831\n",
      "Epoch:234, batch_idx:34/157, Duration:0.02, Top-1 Acc:0.7812\n",
      "Epoch:234, batch_idx:35/157, Duration:0.02, Top-1 Acc:0.7834\n",
      "Epoch:234, batch_idx:36/157, Duration:0.02, Top-1 Acc:0.7829\n",
      "Epoch:234, batch_idx:37/157, Duration:0.02, Top-1 Acc:0.7833\n",
      "Epoch:234, batch_idx:38/157, Duration:0.02, Top-1 Acc:0.7841\n",
      "Epoch:234, batch_idx:39/157, Duration:0.02, Top-1 Acc:0.7852\n",
      "Epoch:234, batch_idx:40/157, Duration:0.02, Top-1 Acc:0.7851\n",
      "Epoch:234, batch_idx:41/157, Duration:0.02, Top-1 Acc:0.7853\n",
      "Epoch:234, batch_idx:42/157, Duration:0.02, Top-1 Acc:0.7849\n",
      "Epoch:234, batch_idx:43/157, Duration:0.02, Top-1 Acc:0.7834\n",
      "Epoch:234, batch_idx:44/157, Duration:0.02, Top-1 Acc:0.7812\n",
      "Epoch:234, batch_idx:45/157, Duration:0.02, Top-1 Acc:0.7789\n",
      "Epoch:234, batch_idx:46/157, Duration:0.02, Top-1 Acc:0.7786\n",
      "Epoch:234, batch_idx:47/157, Duration:0.02, Top-1 Acc:0.7770\n",
      "Epoch:234, batch_idx:48/157, Duration:0.02, Top-1 Acc:0.7768\n",
      "Epoch:234, batch_idx:49/157, Duration:0.02, Top-1 Acc:0.7781\n",
      "Epoch:234, batch_idx:50/157, Duration:0.02, Top-1 Acc:0.7776\n",
      "Epoch:234, batch_idx:51/157, Duration:0.02, Top-1 Acc:0.7782\n",
      "Epoch:234, batch_idx:52/157, Duration:0.02, Top-1 Acc:0.7777\n",
      "Epoch:234, batch_idx:53/157, Duration:0.02, Top-1 Acc:0.7778\n",
      "Epoch:234, batch_idx:54/157, Duration:0.02, Top-1 Acc:0.7761\n",
      "Epoch:234, batch_idx:55/157, Duration:0.02, Top-1 Acc:0.7762\n",
      "Epoch:234, batch_idx:56/157, Duration:0.02, Top-1 Acc:0.7766\n",
      "Epoch:234, batch_idx:57/157, Duration:0.02, Top-1 Acc:0.7772\n",
      "Epoch:234, batch_idx:58/157, Duration:0.02, Top-1 Acc:0.7765\n",
      "Epoch:234, batch_idx:59/157, Duration:0.02, Top-1 Acc:0.7773\n",
      "Epoch:234, batch_idx:60/157, Duration:0.02, Top-1 Acc:0.7766\n",
      "Epoch:234, batch_idx:61/157, Duration:0.02, Top-1 Acc:0.7782\n",
      "Epoch:234, batch_idx:62/157, Duration:0.02, Top-1 Acc:0.7780\n",
      "Epoch:234, batch_idx:63/157, Duration:0.02, Top-1 Acc:0.7776\n",
      "Epoch:234, batch_idx:64/157, Duration:0.02, Top-1 Acc:0.7779\n",
      "Epoch:234, batch_idx:65/157, Duration:0.02, Top-1 Acc:0.7772\n",
      "Epoch:234, batch_idx:66/157, Duration:0.02, Top-1 Acc:0.7778\n",
      "Epoch:234, batch_idx:67/157, Duration:0.02, Top-1 Acc:0.7776\n",
      "Epoch:234, batch_idx:68/157, Duration:0.02, Top-1 Acc:0.7781\n",
      "Epoch:234, batch_idx:69/157, Duration:0.02, Top-1 Acc:0.7792\n",
      "Epoch:234, batch_idx:70/157, Duration:0.02, Top-1 Acc:0.7795\n",
      "Epoch:234, batch_idx:71/157, Duration:0.02, Top-1 Acc:0.7793\n",
      "Epoch:234, batch_idx:72/157, Duration:0.02, Top-1 Acc:0.7787\n",
      "Epoch:234, batch_idx:73/157, Duration:0.02, Top-1 Acc:0.7793\n",
      "Epoch:234, batch_idx:74/157, Duration:0.02, Top-1 Acc:0.7792\n",
      "Epoch:234, batch_idx:75/157, Duration:0.02, Top-1 Acc:0.7792\n",
      "Epoch:234, batch_idx:76/157, Duration:0.02, Top-1 Acc:0.7786\n",
      "Epoch:234, batch_idx:77/157, Duration:0.02, Top-1 Acc:0.7792\n",
      "Epoch:234, batch_idx:78/157, Duration:0.02, Top-1 Acc:0.7789\n",
      "Epoch:234, batch_idx:79/157, Duration:0.02, Top-1 Acc:0.7781\n",
      "Epoch:234, batch_idx:80/157, Duration:0.02, Top-1 Acc:0.7785\n",
      "Epoch:234, batch_idx:81/157, Duration:0.02, Top-1 Acc:0.7788\n",
      "Epoch:234, batch_idx:82/157, Duration:0.02, Top-1 Acc:0.7784\n",
      "Epoch:234, batch_idx:83/157, Duration:0.02, Top-1 Acc:0.7785\n",
      "Epoch:234, batch_idx:84/157, Duration:0.02, Top-1 Acc:0.7785\n",
      "Epoch:234, batch_idx:85/157, Duration:0.02, Top-1 Acc:0.7783\n",
      "Epoch:234, batch_idx:86/157, Duration:0.02, Top-1 Acc:0.7780\n",
      "Epoch:234, batch_idx:87/157, Duration:0.02, Top-1 Acc:0.7786\n",
      "Epoch:234, batch_idx:88/157, Duration:0.02, Top-1 Acc:0.7784\n",
      "Epoch:234, batch_idx:89/157, Duration:0.02, Top-1 Acc:0.7783\n",
      "Epoch:234, batch_idx:90/157, Duration:0.02, Top-1 Acc:0.7788\n",
      "Epoch:234, batch_idx:91/157, Duration:0.02, Top-1 Acc:0.7789\n",
      "Epoch:234, batch_idx:92/157, Duration:0.02, Top-1 Acc:0.7789\n",
      "Epoch:234, batch_idx:93/157, Duration:0.02, Top-1 Acc:0.7786\n",
      "Epoch:234, batch_idx:94/157, Duration:0.02, Top-1 Acc:0.7786\n",
      "Epoch:234, batch_idx:95/157, Duration:0.02, Top-1 Acc:0.7783\n",
      "Epoch:234, batch_idx:96/157, Duration:0.02, Top-1 Acc:0.7784\n",
      "Epoch:234, batch_idx:97/157, Duration:0.02, Top-1 Acc:0.7789\n",
      "Epoch:234, batch_idx:98/157, Duration:0.02, Top-1 Acc:0.7794\n",
      "Epoch:234, batch_idx:99/157, Duration:0.02, Top-1 Acc:0.7795\n",
      "Epoch:234, batch_idx:100/157, Duration:0.02, Top-1 Acc:0.7805\n",
      "Epoch:234, batch_idx:101/157, Duration:0.02, Top-1 Acc:0.7793\n",
      "Epoch:234, batch_idx:102/157, Duration:0.02, Top-1 Acc:0.7779\n",
      "Epoch:234, batch_idx:103/157, Duration:0.02, Top-1 Acc:0.7782\n",
      "Epoch:234, batch_idx:104/157, Duration:0.02, Top-1 Acc:0.7784\n",
      "Epoch:234, batch_idx:105/157, Duration:0.02, Top-1 Acc:0.7784\n",
      "Epoch:234, batch_idx:106/157, Duration:0.02, Top-1 Acc:0.7791\n",
      "Epoch:234, batch_idx:107/157, Duration:0.02, Top-1 Acc:0.7789\n",
      "Epoch:234, batch_idx:108/157, Duration:0.02, Top-1 Acc:0.7794\n",
      "Epoch:234, batch_idx:109/157, Duration:0.02, Top-1 Acc:0.7793\n",
      "Epoch:234, batch_idx:110/157, Duration:0.02, Top-1 Acc:0.7784\n",
      "Epoch:234, batch_idx:111/157, Duration:0.02, Top-1 Acc:0.7789\n",
      "Epoch:234, batch_idx:112/157, Duration:0.02, Top-1 Acc:0.7792\n",
      "Epoch:234, batch_idx:113/157, Duration:0.02, Top-1 Acc:0.7795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:234, batch_idx:114/157, Duration:0.02, Top-1 Acc:0.7793\n",
      "Epoch:234, batch_idx:115/157, Duration:0.02, Top-1 Acc:0.7792\n",
      "Epoch:234, batch_idx:116/157, Duration:0.02, Top-1 Acc:0.7786\n",
      "Epoch:234, batch_idx:117/157, Duration:0.02, Top-1 Acc:0.7790\n",
      "Epoch:234, batch_idx:118/157, Duration:0.02, Top-1 Acc:0.7789\n",
      "Epoch:234, batch_idx:119/157, Duration:0.02, Top-1 Acc:0.7786\n",
      "Epoch:234, batch_idx:120/157, Duration:0.02, Top-1 Acc:0.7782\n",
      "Epoch:234, batch_idx:121/157, Duration:0.02, Top-1 Acc:0.7778\n",
      "Epoch:234, batch_idx:122/157, Duration:0.02, Top-1 Acc:0.7772\n",
      "Epoch:234, batch_idx:123/157, Duration:0.02, Top-1 Acc:0.7772\n",
      "Epoch:234, batch_idx:124/157, Duration:0.02, Top-1 Acc:0.7771\n",
      "Epoch:234, batch_idx:125/157, Duration:0.02, Top-1 Acc:0.7770\n",
      "Epoch:234, batch_idx:126/157, Duration:0.02, Top-1 Acc:0.7773\n",
      "Epoch:234, batch_idx:127/157, Duration:0.02, Top-1 Acc:0.7773\n",
      "Epoch:234, batch_idx:128/157, Duration:0.02, Top-1 Acc:0.7773\n",
      "Epoch:234, batch_idx:129/157, Duration:0.02, Top-1 Acc:0.7774\n",
      "Epoch:234, batch_idx:130/157, Duration:0.02, Top-1 Acc:0.7772\n",
      "Epoch:234, batch_idx:131/157, Duration:0.02, Top-1 Acc:0.7775\n",
      "Epoch:234, batch_idx:132/157, Duration:0.02, Top-1 Acc:0.7776\n",
      "Epoch:234, batch_idx:133/157, Duration:0.02, Top-1 Acc:0.7778\n",
      "Epoch:234, batch_idx:134/157, Duration:0.02, Top-1 Acc:0.7777\n",
      "Epoch:234, batch_idx:135/157, Duration:0.02, Top-1 Acc:0.7779\n",
      "Epoch:234, batch_idx:136/157, Duration:0.02, Top-1 Acc:0.7787\n",
      "Epoch:234, batch_idx:137/157, Duration:0.02, Top-1 Acc:0.7782\n",
      "Epoch:234, batch_idx:138/157, Duration:0.02, Top-1 Acc:0.7784\n",
      "Epoch:234, batch_idx:139/157, Duration:0.02, Top-1 Acc:0.7781\n",
      "Epoch:234, batch_idx:140/157, Duration:0.02, Top-1 Acc:0.7783\n",
      "Epoch:234, batch_idx:141/157, Duration:0.02, Top-1 Acc:0.7781\n",
      "Epoch:234, batch_idx:142/157, Duration:0.02, Top-1 Acc:0.7774\n",
      "Epoch:234, batch_idx:143/157, Duration:0.02, Top-1 Acc:0.7782\n",
      "Epoch:234, batch_idx:144/157, Duration:0.02, Top-1 Acc:0.7778\n",
      "Epoch:234, batch_idx:145/157, Duration:0.02, Top-1 Acc:0.7778\n",
      "Epoch:234, batch_idx:146/157, Duration:0.02, Top-1 Acc:0.7780\n",
      "Epoch:234, batch_idx:147/157, Duration:0.02, Top-1 Acc:0.7773\n",
      "Epoch:234, batch_idx:148/157, Duration:0.02, Top-1 Acc:0.7782\n",
      "Epoch:234, batch_idx:149/157, Duration:0.02, Top-1 Acc:0.7782\n",
      "Epoch:234, batch_idx:150/157, Duration:0.02, Top-1 Acc:0.7785\n",
      "Epoch:234, batch_idx:151/157, Duration:0.02, Top-1 Acc:0.7787\n",
      "Epoch:234, batch_idx:152/157, Duration:0.02, Top-1 Acc:0.7789\n",
      "Epoch:234, batch_idx:153/157, Duration:0.02, Top-1 Acc:0.7794\n",
      "Epoch:234, batch_idx:154/157, Duration:0.02, Top-1 Acc:0.7794\n",
      "Epoch:234, batch_idx:155/157, Duration:0.02, Top-1 Acc:0.7794\n",
      "Epoch:234, batch_idx:156/157, Duration:0.01, Top-1 Acc:0.7794\n",
      "test epoch:234\n",
      "Test Top-1 ss_accuracy: [0.6145, 0.6112, 0.6265]\n",
      "Test Top-1 class_accuracy: [0.629, 0.6273, 0.6635, 0.7794]\n",
      "\n",
      "best_accuracy: 0.7764 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.  \n",
    "start_epoch = 0  \n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_div = DistillKL(kd_T)\n",
    "args = {'warmup_epoch': warmup_epoch, 'init_lr': init_lr, 'milestones':milestones}\n",
    "evaluate = False\n",
    "resume = True\n",
    "if evaluate: \n",
    "        print('load pre-trained weights from: {}'.format(os.path.join(checkpoint_dir, str(model.__name__) + '.pth.tar')))     \n",
    "        checkpoint = torch.load(os.path.join(checkpoint_dir, str(model.__name__) + '.pth.tar'),\n",
    "                                map_location=torch.device('cpu'))\n",
    "        net.module.load_state_dict(checkpoint['net'])\n",
    "        best_acc = checkpoint['acc']\n",
    "        test(start_epoch, criterion_cls, net)\n",
    "else:\n",
    "    print('Evaluate Teacher:')\n",
    "    acc = test(0, criterion_cls, tnet)\n",
    "    print('Teacher Acc:', acc)\n",
    "\n",
    "    trainable_list = nn.ModuleList([])\n",
    "    trainable_list.append(net)\n",
    "    optimizer = optim.SGD(trainable_list.parameters(),\n",
    "                          lr=0.1, momentum=0.9, weight_decay=weight_decay, nesterov=True)\n",
    "\n",
    "    criterion_list = nn.ModuleList([])\n",
    "    criterion_list.append(criterion_cls)  # classification loss\n",
    "    criterion_list.append(criterion_div)  # KL divergence loss, original knowledge distillation\n",
    "    criterion_list.cuda()\n",
    "\n",
    "\n",
    "    if resume:\n",
    "        print('load pre-trained weights from: {}'.format(os.path.join(checkpoint_dir, str(model.__name__) + '.pth.tar')))\n",
    "        checkpoint = torch.load(os.path.join(checkpoint_dir, str(model.__name__) + '.pth.tar'),\n",
    "                                map_location=torch.device('cpu'))\n",
    "        net.module.load_state_dict(checkpoint['net'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        best_acc = checkpoint['acc']\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        train(epoch, criterion_list, optimizer)\n",
    "        acc = test(epoch, criterion_cls, net)\n",
    "\n",
    "        state = {\n",
    "            'net': net.module.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(state, os.path.join(checkpoint_dir, str(model.__name__) + '.pth.tar'))\n",
    "\n",
    "        is_best = False\n",
    "        if best_acc < acc:\n",
    "            best_acc = acc\n",
    "            is_best = True\n",
    "\n",
    "        if is_best:\n",
    "            shutil.copyfile(os.path.join(checkpoint_dir, str(model.__name__) + '.pth.tar'),\n",
    "                            os.path.join(checkpoint_dir, str(model.__name__) + '_best.pth.tar'))\n",
    "\n",
    "    print('Evaluate the best model:')\n",
    "    print('load pre-trained weights from: {}'.format(os.path.join(checkpoint_dir, str(model.__name__) + '_best.pth.tar')))\n",
    "    evaluate = True\n",
    "    checkpoint = torch.load(os.path.join(checkpoint_dir, str(model.__name__) + '_best.pth.tar'),\n",
    "                            map_location=torch.device('cpu'))\n",
    "    net.module.load_state_dict(checkpoint['net'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    top1_acc = test(start_epoch, criterion_cls, net)\n",
    "\n",
    "    with open(log_txt, 'a+') as f:\n",
    "        f.write('best_accuracy: {} \\n'.format(best_acc))\n",
    "        print('best_accuracy: {} \\n'.format(best_acc))\n",
    "        os.system('cp ' + log_txt + ' ' + checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab4f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
